{
  "peft_kwargs": {
    "r": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.05,
    "bias": "none",
    "task_type": "CAUSAL_LM"
  },
  "ppo_kwargs": {
      "batch_size": 4,              
      "mini_batch_size": 1,
      "gradient_accumulation_steps": 4,
      "ppo_epochs": 5,
      "learning_rate": 1e-5,
      "log_with": "wandb"
  }
}